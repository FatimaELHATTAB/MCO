from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterator, Mapping, Optional, Tuple

import polars as pl
import psycopg
import yaml


# ---------- Config models (simple V1) ----------

@dataclass(frozen=True)
class DatasetSpec:
    query: str
    schema_yaml: Dict[str, str]  # e.g. {"col": "Utf8"}


@dataclass(frozen=True)
class ProviderSpec:
    provider: str
    chunk_size: int
    params_defaults: Dict[str, Any]
    normalized: DatasetSpec
    identity: DatasetSpec


def _yaml_to_polars_schema(schema_yaml: Mapping[str, str]) -> Dict[str, pl.DataType]:
    out: Dict[str, pl.DataType] = {}
    for col, dtype_name in schema_yaml.items():
        try:
            out[col] = getattr(pl, str(dtype_name))
        except AttributeError as e:
            raise ValueError(f"Unknown Polars dtype '{dtype_name}' for column '{col}'") from e
    return out


def _load_provider_spec(config_dir: str | Path, provider: str) -> ProviderSpec:
    path = Path(config_dir) / f"{provider}.yaml"
    raw = yaml.safe_load(path.read_text(encoding="utf-8")) or {}

    if raw.get("provider") != provider:
        raise ValueError(f"Config provider mismatch in {path}: expected '{provider}'")

    chunk_size = int((raw.get("read") or {}).get("chunk_size", 50_000))
    params_defaults = (raw.get("read") or {}).get("params_defaults") or {}

    datasets = raw.get("datasets") or {}
    if "normalized" not in datasets or "identity" not in datasets:
        raise ValueError(f"Missing datasets.normalized or datasets.identity in {path}")

    norm = datasets["normalized"]
    ident = datasets["identity"]

    def parse_dataset(ds: dict) -> DatasetSpec:
        q = (ds.get("query") or "").strip()
        if not q:
            raise ValueError(f"Empty dataset query in {path}")
        schema_yaml = ds.get("schema") or {}
        if not schema_yaml:
            raise ValueError(f"Missing dataset schema in {path}")
        return DatasetSpec(query=q, schema_yaml=schema_yaml)

    return ProviderSpec(
        provider=provider,
        chunk_size=chunk_size,
        params_defaults=params_defaults,
        normalized=parse_dataset(norm),
        identity=parse_dataset(ident),
    )


# ---------- DB Reader (chunked) ----------

def _iter_query_chunks(
    conn: psycopg.Connection,
    query: str,
    params: Mapping[str, Any],
    schema: Dict[str, pl.DataType],
    chunk_size: int,
) -> Iterator[pl.DataFrame]:
    # server-side cursor to avoid loading everything at once
    with conn.cursor(name="cur_polars") as cur:
        cur.execute(query, params)
        colnames = [d.name for d in cur.description]  # type: ignore[attr-defined]

        while True:
            rows = cur.fetchmany(chunk_size)
            if not rows:
                break

            # Build DataFrame from rows + explicit schema
            # orient="row" keeps it simple and stable
            yield pl.DataFrame(rows, schema=colnames).cast(schema, strict=False)


def _read_query_as_df(
    conn: psycopg.Connection,
    query: str,
    params: Mapping[str, Any],
    schema: Dict[str, pl.DataType],
    chunk_size: int,
) -> pl.DataFrame:
    chunks = list(_iter_query_chunks(conn, query, params, schema, chunk_size))
    if not chunks:
        # empty DF with schema
        return pl.DataFrame(schema=schema)
    return pl.concat(chunks, how="vertical", rechunk=True)


# ---------- DataLoader ----------

class DataLoader:
    def __init__(self, dsn: str, config_dir: str | Path) -> None:
        self._dsn = dsn
        self._config_dir = Path(config_dir)

    def load(self, provider: str, params_override: Optional[Mapping[str, Any]] = None) -> Tuple[pl.DataFrame, pl.DataFrame]:
        spec = _load_provider_spec(self._config_dir, provider)

        # merge params defaults + overrides (V1)
        params: Dict[str, Any] = dict(spec.params_defaults)
        if params_override:
            params.update(params_override)

        norm_schema = _yaml_to_polars_schema(spec.normalized.schema_yaml)
        ident_schema = _yaml_to_polars_schema(spec.identity.schema_yaml)

        with psycopg.connect(self._dsn) as conn:
            df_normalized = _read_query_as_df(
                conn=conn,
                query=spec.normalized.query,
                params=params,
                schema=norm_schema,
                chunk_size=spec.chunk_size,
            )
            df_identity = _read_query_as_df(
                conn=conn,
                query=spec.identity.query,
                params=params,
                schema=ident_schema,
                chunk_size=spec.chunk_size,
            )

        return df_normalized, df_identity
